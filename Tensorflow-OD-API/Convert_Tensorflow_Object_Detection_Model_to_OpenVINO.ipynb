{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Convert Tensorflow Object Detection Model to OpenVINO.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IUsiOvFrF5c"
      },
      "source": [
        "## Installation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgR_6bFKq7Il",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdf96044-3bb5-4d68-f761-8cf968395eff"
      },
      "source": [
        "%tensorflow_version 1.x\r\n",
        "!pip install tf_slim"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "Collecting tf_slim\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/97/b0f4a64df018ca018cc035d44f2ef08f91e2e8aa67271f6f19633a015ff7/tf_slim-1.1.0-py2.py3-none-any.whl (352kB)\n",
            "\u001b[K     |████████████████████████████████| 358kB 8.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from tf_slim) (0.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from absl-py>=0.2.2->tf_slim) (1.15.0)\n",
            "Installing collected packages: tf-slim\n",
            "Successfully installed tf-slim-1.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6QBO9p2ArAbp"
      },
      "source": [
        "%%capture\n",
        "%cd /content\n",
        "!git clone --quiet https://github.com/tensorflow/models.git\n",
        "%cd /content/models/\n",
        "!git checkout 58d19c67e1d30d905dd5c6e5092348658fed80af\n",
        "!apt-get install -qq protobuf-compiler python-pil python-lxml python-tk\n",
        "!pip install -q Cython contextlib2 pillow lxml matplotlib\n",
        "!pip install -q pycocotools\n",
        "%cd /content/models/research\n",
        "!protoc object_detection/protos/*.proto --python_out=.\n",
        "import os\n",
        "os.environ['PYTHONPATH'] += ':/content/models/research/:/content/models/research/slim/'\n",
        "!python object_detection/builders/model_builder_test.py\n",
        "%cd /content"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gws9Q3kcrE4i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0666de5-8dce-41ce-c314-bdac5437994a"
      },
      "source": [
        "#run model builder test\n",
        "!python /content/models/research/object_detection/builders/model_builder_tf1_test.py"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running tests under Python 3.6.9: /usr/bin/python3\n",
            "[ RUN      ] ModelBuilderTF1Test.test_create_context_rcnn_from_config_with_params0 (True)\n",
            "[       OK ] ModelBuilderTF1Test.test_create_context_rcnn_from_config_with_params0 (True)\n",
            "[ RUN      ] ModelBuilderTF1Test.test_create_context_rcnn_from_config_with_params1 (False)\n",
            "[       OK ] ModelBuilderTF1Test.test_create_context_rcnn_from_config_with_params1 (False)\n",
            "[ RUN      ] ModelBuilderTF1Test.test_create_experimental_model\n",
            "[       OK ] ModelBuilderTF1Test.test_create_experimental_model\n",
            "[ RUN      ] ModelBuilderTF1Test.test_create_faster_rcnn_from_config_with_crop_feature0 (True)\n",
            "[       OK ] ModelBuilderTF1Test.test_create_faster_rcnn_from_config_with_crop_feature0 (True)\n",
            "[ RUN      ] ModelBuilderTF1Test.test_create_faster_rcnn_from_config_with_crop_feature1 (False)\n",
            "[       OK ] ModelBuilderTF1Test.test_create_faster_rcnn_from_config_with_crop_feature1 (False)\n",
            "[ RUN      ] ModelBuilderTF1Test.test_create_faster_rcnn_model_from_config_with_example_miner\n",
            "[       OK ] ModelBuilderTF1Test.test_create_faster_rcnn_model_from_config_with_example_miner\n",
            "[ RUN      ] ModelBuilderTF1Test.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul\n",
            "[       OK ] ModelBuilderTF1Test.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul\n",
            "[ RUN      ] ModelBuilderTF1Test.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul\n",
            "[       OK ] ModelBuilderTF1Test.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul\n",
            "[ RUN      ] ModelBuilderTF1Test.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul\n",
            "[       OK ] ModelBuilderTF1Test.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul\n",
            "[ RUN      ] ModelBuilderTF1Test.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul\n",
            "[       OK ] ModelBuilderTF1Test.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul\n",
            "[ RUN      ] ModelBuilderTF1Test.test_create_rfcn_model_from_config\n",
            "[       OK ] ModelBuilderTF1Test.test_create_rfcn_model_from_config\n",
            "[ RUN      ] ModelBuilderTF1Test.test_create_ssd_fpn_model_from_config\n",
            "[       OK ] ModelBuilderTF1Test.test_create_ssd_fpn_model_from_config\n",
            "[ RUN      ] ModelBuilderTF1Test.test_create_ssd_models_from_config\n",
            "[       OK ] ModelBuilderTF1Test.test_create_ssd_models_from_config\n",
            "[ RUN      ] ModelBuilderTF1Test.test_invalid_faster_rcnn_batchnorm_update\n",
            "[       OK ] ModelBuilderTF1Test.test_invalid_faster_rcnn_batchnorm_update\n",
            "[ RUN      ] ModelBuilderTF1Test.test_invalid_first_stage_nms_iou_threshold\n",
            "[       OK ] ModelBuilderTF1Test.test_invalid_first_stage_nms_iou_threshold\n",
            "[ RUN      ] ModelBuilderTF1Test.test_invalid_model_config_proto\n",
            "[       OK ] ModelBuilderTF1Test.test_invalid_model_config_proto\n",
            "[ RUN      ] ModelBuilderTF1Test.test_invalid_second_stage_batch_size\n",
            "[       OK ] ModelBuilderTF1Test.test_invalid_second_stage_batch_size\n",
            "[ RUN      ] ModelBuilderTF1Test.test_session\n",
            "[  SKIPPED ] ModelBuilderTF1Test.test_session\n",
            "[ RUN      ] ModelBuilderTF1Test.test_unknown_faster_rcnn_feature_extractor\n",
            "[       OK ] ModelBuilderTF1Test.test_unknown_faster_rcnn_feature_extractor\n",
            "[ RUN      ] ModelBuilderTF1Test.test_unknown_meta_architecture\n",
            "[       OK ] ModelBuilderTF1Test.test_unknown_meta_architecture\n",
            "[ RUN      ] ModelBuilderTF1Test.test_unknown_ssd_feature_extractor\n",
            "[       OK ] ModelBuilderTF1Test.test_unknown_ssd_feature_extractor\n",
            "----------------------------------------------------------------------\n",
            "Ran 21 tests in 0.192s\n",
            "\n",
            "OK (skipped=1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5x_vhSUJsdPx"
      },
      "source": [
        "## Downloading model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8UqgfoTse-6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e325781-e2db-4aa6-cd46-7eb34ef8f8ec"
      },
      "source": [
        "!wget http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v2_coco_2018_03_29.tar.gz\n",
        "!tar -xf ssd_mobilenet_v2_coco_2018_03_29.tar.gz"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-02-03 18:38:04--  http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v2_coco_2018_03_29.tar.gz\n",
            "Resolving download.tensorflow.org (download.tensorflow.org)... 74.125.142.128, 2607:f8b0:400e:c08::80\n",
            "Connecting to download.tensorflow.org (download.tensorflow.org)|74.125.142.128|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 187925923 (179M) [application/x-tar]\n",
            "Saving to: ‘ssd_mobilenet_v2_coco_2018_03_29.tar.gz’\n",
            "\n",
            "ssd_mobilenet_v2_co 100%[===================>] 179.22M  54.5MB/s    in 3.3s    \n",
            "\n",
            "2021-02-03 18:38:08 (54.5 MB/s) - ‘ssd_mobilenet_v2_coco_2018_03_29.tar.gz’ saved [187925923/187925923]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bq5snLqKRsBm"
      },
      "source": [
        "## Install OpenVINO 20.01"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MGjyhzfLRnEF",
        "outputId": "d4c4e217-5b9d-4ef1-8978-89073e56df41"
      },
      "source": [
        "%%time\r\n",
        "%%capture\r\n",
        "## install tools. Open Vino takes some time to download: 10-15 min sometimes.\r\n",
        "!sudo apt-get install -y pciutils cpio\r\n",
        "!sudo apt autoremove\r\n",
        "## download installation files\r\n",
        "!wget http://registrationcenter-download.intel.com/akdlm/irc_nas/16345/l_openvino_toolkit_p_2020.1.023.tgz\r\n",
        "path = 'l_openvino_toolkit_p_2020.1.023.tgz'\r\n",
        "# path = \"/content/software/Intel OpenVINO 2019 R3.1/l_openvino_toolkit_p_2019.3.376.tgz\"\r\n",
        "## install openvino\r\n",
        "!tar xf '{path}'\r\n",
        "%cd l_openvino_toolkit_p_2020.1.023/\r\n",
        "!./install_openvino_dependencies.sh && \\\r\n",
        "    sed -i 's/decline/accept/g' silent.cfg && \\\r\n",
        "    ./install.sh --silent silent.cfg"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 288 ms, sys: 53.6 ms, total: 341 ms\n",
            "Wall time: 1min 45s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0nB86o2SA2e"
      },
      "source": [
        "## Convert TF model to OpenVINO IR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hszJ1jYESC23",
        "outputId": "09154833-29cb-4ce9-d802-d1fbf54731f7"
      },
      "source": [
        "output_dir = '/content/output'\r\n",
        "\r\n",
        "%cd '/content/ssd_mobilenet_v2_coco_2018_03_29/'\r\n",
        "!source /opt/intel/openvino/bin/setupvars.sh && \\\r\n",
        "    python /opt/intel/openvino/deployment_tools/model_optimizer/mo.py \\\r\n",
        "    --input_model frozen_inference_graph.pb \\\r\n",
        "    --tensorflow_use_custom_operations_config /opt/intel/openvino/deployment_tools/model_optimizer/extensions/front/tf/ssd_v2_support.json \\\r\n",
        "    --tensorflow_object_detection_api_pipeline_config pipeline.config \\\r\n",
        "    --reverse_input_channels \\\r\n",
        "    --output_dir {output_dir} \\\r\n",
        "    --data_type FP16  "
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/ssd_mobilenet_v2_coco_2018_03_29\n",
            "[setupvars.sh] OpenVINO environment initialized\n",
            "[ WARNING ]  Use of deprecated cli option --tensorflow_use_custom_operations_config detected. Option use in the following releases will be fatal. Please use --transformations_config cli option instead\n",
            "Model Optimizer arguments:\n",
            "Common parameters:\n",
            "\t- Path to the Input Model: \t/content/ssd_mobilenet_v2_coco_2018_03_29/frozen_inference_graph.pb\n",
            "\t- Path for generated IR: \t/content/output\n",
            "\t- IR output name: \tfrozen_inference_graph\n",
            "\t- Log level: \tERROR\n",
            "\t- Batch: \tNot specified, inherited from the model\n",
            "\t- Input layers: \tNot specified, inherited from the model\n",
            "\t- Output layers: \tNot specified, inherited from the model\n",
            "\t- Input shapes: \tNot specified, inherited from the model\n",
            "\t- Mean values: \tNot specified\n",
            "\t- Scale values: \tNot specified\n",
            "\t- Scale factor: \tNot specified\n",
            "\t- Precision of IR: \tFP16\n",
            "\t- Enable fusing: \tTrue\n",
            "\t- Enable grouped convolutions fusing: \tTrue\n",
            "\t- Move mean values to preprocess section: \tFalse\n",
            "\t- Reverse input channels: \tTrue\n",
            "TensorFlow specific parameters:\n",
            "\t- Input model in text protobuf format: \tFalse\n",
            "\t- Path to model dump for TensorBoard: \tNone\n",
            "\t- List of shared libraries with TensorFlow custom layers implementation: \tNone\n",
            "\t- Update the configuration file with input/output node names: \tNone\n",
            "\t- Use configuration file used to generate the model with Object Detection API: \t/content/ssd_mobilenet_v2_coco_2018_03_29/pipeline.config\n",
            "\t- Operations to offload: \tNone\n",
            "\t- Patterns to offload: \tNone\n",
            "\t- Use the config file: \t/opt/intel/openvino/deployment_tools/model_optimizer/extensions/front/tf/ssd_v2_support.json\n",
            "Model Optimizer version: \t2020.1.0-61-gd349c3ba4a\n",
            "The Preprocessor block has been removed. Only nodes performing mean value subtraction and scaling (if applicable) are kept.\n",
            "\n",
            "[ SUCCESS ] Generated IR version 10 model.\n",
            "[ SUCCESS ] XML file: /content/output/frozen_inference_graph.xml\n",
            "[ SUCCESS ] BIN file: /content/output/frozen_inference_graph.bin\n",
            "[ SUCCESS ] Total execution time: 53.44 seconds. \n",
            "[ SUCCESS ] Memory consumed: 714 MB. \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eF737-1wb98X"
      },
      "source": [
        "## Compile the IR model to a .blob for use on DepthAI modules/platform"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J06WS2MpcC0i"
      },
      "source": [
        "import requests\r\n",
        "\r\n",
        "url = 'http://69.164.214.171:8083/compile'  # change if running against other URL\r\n",
        "\r\n",
        "payload = {\r\n",
        "    'compiler_params': '-ip U8 -VPU_MYRIAD_PLATFORM VPU_MYRIAD_2480 -VPU_NUMBER_OF_SHAVES 4 -VPU_NUMBER_OF_CMX_SLICES 4',\r\n",
        "    'compile_type': 'myriad'\r\n",
        "}\r\n",
        "files = {\r\n",
        "    'definition': open(f'{output_dir}/frozen_inference_graph.xml', 'rb'),\r\n",
        "    'weights': open(f'{output_dir}/frozen_inference_graph.bin', 'rb')\r\n",
        "}\r\n",
        "params = {\r\n",
        "    'version': '2020.1',  # OpenVINO version, can be \"2021.1\", \"2020.4\", \"2020.3\", \"2020.2\", \"2020.1\", \"2019.R3\"\r\n",
        "}\r\n",
        "\r\n",
        "response = requests.post(url, data=payload, files=files, params=params)\r\n",
        "\r\n",
        "with open(f'{output_dir}/model.blob', 'wb') as f:\r\n",
        "  f.write(response.content)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "l37N8fVNdDDO",
        "outputId": "98954547-1e85-4ca0-994a-6f36ee09d991"
      },
      "source": [
        "from google.colab import files\r\n",
        "files.download(f'{output_dir}/model.blob')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_49063752-a294-43dd-aa6b-60b289091a86\", \"model.blob\", 38766848)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}